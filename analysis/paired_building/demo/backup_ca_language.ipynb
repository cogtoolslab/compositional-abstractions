{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ef775",
   "metadata": {},
   "source": [
    "# Linguistic analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import num2words\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c4619",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd94690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dataframes from each eventType\n",
    "df_chat = pd.read_csv('../../results/csv/df_chat_cogsci21.csv')\n",
    "df_trial = pd.read_csv('../../results/csv/df_trial_cogsci21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b80ac",
   "metadata": {},
   "source": [
    "#### Remove datasets that didn't meet accuracy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% Accuracy on 75% of trials\n",
    "df75 = pd.DataFrame(df_trial.groupby(['gameid', 'trialNum'])['trialScore'].sum()>75).groupby(['gameid']).sum()\n",
    "df75['trials'] = df75['trialScore']\n",
    "\n",
    "df75 = df75[df75['trials']>=9]\n",
    "includedGames = list(df75.reset_index().gameid)\n",
    "\n",
    "print(\"Total dyads achieving 75% Accuracy on 75% of trials:\",len(df75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e666475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude from analysis\n",
    "df_chat = df_chat[df_chat.gameid.isin(includedGames)]\n",
    "df_trial = df_trial[df_trial.gameid.isin(includedGames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e10f0",
   "metadata": {},
   "source": [
    "## Basic linguistic analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for char and word counts\n",
    "df_chat['word_count'] = df_chat['content'].str.split(' ').str.len()\n",
    "df_chat['char_count'] = df_chat['content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31affe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chat[\"timeElapsedInTurn\"] = pd.to_numeric(df_chat['timeElapsedInTurn'])\n",
    "\n",
    "# add to trial df\n",
    "trial_sums = df_chat[['gameid','trialNum','word_count','char_count']].groupby(['gameid','trialNum']).sum().reset_index()\n",
    "df_trial = df_trial.merge(trial_sums, how='outer',on=['gameid','trialNum'])\n",
    "\n",
    "# message countsa\n",
    "counts = df_chat.groupby(['gameid','trialNum'])[['iterationName']].count().reset_index()\\\n",
    "    .rename(columns={'iterationName':'n_messages'})\n",
    "df_trial = df_trial.merge(counts, how='left', on=['gameid','trialNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word/char count over repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3fff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26b89aea",
   "metadata": {},
   "source": [
    "## Analysing referring expressions \n",
    "\n",
    "While our analyses so far tell us about language use in general, we're primarily interested in how the expressions used to refer to entities in our experiment change over time.  \n",
    "In particular, we want to know when people transition from providing instructions about lower-level, block by block placements, to higher-level tower abstractions.  \n",
    "While we could in principle use NLP techniques to extract noun phrases and assess their meaning, people might use a wide variety of expressions to refer to blocks and towers.  \n",
    "\n",
    "We therefore asked naive raters to **identify the referring expressions** used in each message, as well as the **number of abstractions at each level (block vs. tower)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c4507",
   "metadata": {},
   "source": [
    "### Load referring expression annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps = pd.read_csv('data/df_ref_exps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56909813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84de4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at collection of referring expressions identified by raters\n",
    "df_ref_exps[['message','content','block','tower']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps.loc[:,'content'] = df_ref_exps.loc[:,'content'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae72b9",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea941bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_ref_exps['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert number words\n",
    "\n",
    "def num_2_words(sentence):\n",
    "    out = \"\"\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            o = num2words(word)\n",
    "        except:\n",
    "            o = word\n",
    "        out = out+\" \"+ o\n",
    "    return out\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: num_2_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)]\n",
    "\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['content'].apply(lemmatize_text)\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: [i.upper() for i in x])\n",
    "\n",
    "df_ref_exps[['message','content','BOW_lemmatized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps['word_freq'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: Counter(x))\n",
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps['BOW_concat'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0a5e7",
   "metadata": {},
   "source": [
    "## Creating distributions of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create support\n",
    "split_words = df_ref_exps['BOW_concat'].apply(lambda x: x.split())\n",
    "all_words = list(pd.Series([st for row in split_words for st in row]).unique())\n",
    "support = {}\n",
    "for word in all_words:\n",
    "    support[word] = 0.000000001\n",
    "    \n",
    "def get_pdist(row):\n",
    "    num_words = np.sum(list(row['word_freq'].values()))\n",
    "    pdist = support.copy()\n",
    "    for i, (word, count) in enumerate(row['word_freq'].items()):\n",
    "        pdist[word] = count/num_words\n",
    "    return pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc183cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps['word_pdist'] = df_ref_exps.apply(get_pdist, axis = 1)\n",
    "df_ref_exps['word_pdist_numeric'] = df_ref_exps['word_pdist'].apply(lambda dist: list(dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words = df_ref_exps[['dyad_gameid', 'rep', 'BOW_concat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in all_words:\n",
    "    df_all_words.loc[:,w] = df_all_words['BOW_concat'].apply(lambda row: int(w in row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_words_reps = df_all_words.groupby('rep').agg(sum)\n",
    "df_all_words_reps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9da79",
   "metadata": {},
   "source": [
    "#### Change in word frequency plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08716f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_exps_rep = df_ref_exps.groupby('rep')['BOW_concat'].apply(lambda group:' '.join(group)).reset_index()\n",
    "df_ref_exps_rep['word_freq'] = df_ref_exps_rep['BOW_concat'].apply(lambda x: Counter(x.split()))\n",
    "df_ref_exps_rep['word_pdist'] = df_ref_exps_rep.apply(get_pdist, axis=1)\n",
    "df_ref_exps_rep['word_pdist_numeric'] = df_ref_exps_rep['word_pdist'].apply(lambda dist: list(dist.values()))\n",
    "df_ref_exps_rep.index=df_ref_exps_rep['rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39439d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in proportion between reps\n",
    "\n",
    "rep_a = 1\n",
    "rep_b = 4\n",
    "\n",
    "rep_diff = {}\n",
    "\n",
    "for _, (k, rep_a_v) in enumerate(df_ref_exps_rep.loc[rep_a,'word_pdist'].items()):\n",
    "    rep_diff[k] = df_ref_exps_rep.loc[rep_b,'word_pdist'][k] - rep_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find largest n increase/ decrease in proportion across reps\n",
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest increase in proportion between reps\n",
    "top_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest decrease in proportion between reps\n",
    "\n",
    "bottom_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=False)[:n])\n",
    "\n",
    "bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_ref_exps.groupby('rep').agg({'BOW_lemmatized': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "font = {'fontname':'Helvetica'}\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "x_limit = 6\n",
    "\n",
    "labels, values = zip(*rep_diff.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort_high = np.argsort(values)[::-1]\n",
    "indSort_low = np.argsort(values)\n",
    "\n",
    "# rearrange your data\n",
    "#labels = np.array(labels)[indSort_high][:x_limit][::-1]\n",
    "labels = np.concatenate([np.array(labels)[indSort_low][:x_limit],np.array(labels)[indSort_high][:x_limit][::-1]])\n",
    "#values = np.array(values)[indSort_high][:x_limit][::-1]\n",
    "values = np.concatenate([np.array(values)[indSort_low][:x_limit], np.array(values)[indSort_high][:x_limit][::-1]])\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(7, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(indexes, values, color = \"#7D7D7D\")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "# add labels\n",
    "plt.yticks(fontsize=16, **font)\n",
    "plt.xticks(indexes + bar_width, labels,  rotation='vertical', fontsize=16, **font)\n",
    "plt.ylabel(\"change in proportion\", size = 24, **font)\n",
    "plt.yticks(np.arange(-.13,.06, .02))\n",
    "ax.axes.get_xaxis().set_visible(True)\n",
    "#plt.title(\"highest delta words\", size = 24, **font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f76339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
