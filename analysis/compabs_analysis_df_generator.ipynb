{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating dataframes from multiple sessions\n",
    "\n",
    "Pilot 3 added more checks for engagement early in the experiment.\n",
    "Iteration names:\n",
    "'pre-pilot3':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in packages and set up server connection\n",
    "This first section will read in necessary packages for anaysis and establish file paths and connections to the mongo db server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"../analysis/utils\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import drawing_utils as drawing\n",
    "import importlib\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "datavol_dir = os.path.join(proj_dir,'data')\n",
    "analysis_dir =  os.path.abspath('.')\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "json_dir = os.path.join(results_dir,'json')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'behavioral_experiments'))\n",
    "png_dir = os.path.abspath(os.path.join(datavol_dir,'png'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'stimuli') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'stimuli'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv(os.path.join(analysis_dir,'auth.txt'), header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['compositional-abstractions']\n",
    "coll = db['two-towers']\n",
    "\n",
    "# which iteration name should we use?\n",
    "iterationName = 'pilot_class_test'\n",
    "\n",
    "## look up number of trials (including paired-practice)\n",
    "numTrials = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into failType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct tidy dataframe with game data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establish connection to mongo\n",
    "First thing you need to do is to establish an ssh tunnel (aka remote port forwarding) to the server, so that requests to the mongodb can be made \"as if\" the mongodb server is running on your local computer. Run this from the command line before you begin data analysis if you plan to fetch data from mongo:\n",
    "\n",
    "`ssh -fNL 27017:127.0.0.1:27017 USERNAME@cogtoolslab.org`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass in list of iterations, returns list of total games and completed games for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 total games in iteration: pilot_class_test.\n",
      "There are 9 complete games in iteration: pilot_class_test.\n"
     ]
    }
   ],
   "source": [
    "## get list of all gameIDs in database\n",
    "#iterationList = ['livetest0','pilot0','pilot2']\n",
    "\n",
    "iterationList = [iterationName]\n",
    "\n",
    "for iteration in iterationList:\n",
    "    #get total games\n",
    "    total_games = coll.find({'iterationName':iteration}).distinct('gameid')\n",
    "    print('There are {} total games in iteration: {}.'.format(len(total_games), iteration))\n",
    "\n",
    "    ## get list of complete gameIDs\n",
    "    gameIDs = coll.find({'iterationName':iteration}).distinct('gameid')\n",
    "    complete_games = [g for g in gameIDs if len(coll.find({'gameid':g}).distinct('trialNum')) == numTrials]\n",
    "    print('There are {} complete games in iteration: {}.'.format(len(complete_games), iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_games = []\n",
    "complete_games = [g for g in complete_games if g not in broken_games]\n",
    "\n",
    "def construct_tidy_dataframe(eventType = 'chatMessage', \n",
    "                             complete_games = [],\n",
    "                             iterationName = 'pilot3',\n",
    "                             remove_workerID = True):\n",
    "    '''\n",
    "    input: list of complete games and name of event Type\n",
    "    '''\n",
    "    event2name = {'chatMessage':'chat', 'block':'block', 'endTrial':'trial', 'exitSurvey':'exit'}\n",
    "    L = pd.DataFrame()\n",
    "    for g, this_gameID in enumerate(complete_games):\n",
    "        print('Analyzing game {} | {} of {}'.format(this_gameID, g+1, len(complete_games)))\n",
    "        clear_output(wait=True) \n",
    "\n",
    "        ### extract records \n",
    "        #loop over iteration names??\n",
    "        X = coll.find({ '$and': [{'iterationName':iterationName}, \n",
    "#                                  {\"$or\":[{'iterationName':'Exp2Pilot3'},\n",
    "#                                  {'iterationName':'Exp2Pilot3_batch2'}]}\n",
    "                                 {'gameid': this_gameID}, {'eventType': eventType}]}).sort('time') \n",
    "        \n",
    "        li = list(X)        \n",
    "        _L = pd.DataFrame(li)  \n",
    "\n",
    "        ## concat with previous game's dataframe\n",
    "        if L.shape[0]==0:\n",
    "            L = _L\n",
    "        else: \n",
    "            L = pd.concat([L,_L], axis=0)     \n",
    "\n",
    "    ## postprocessing\n",
    "    if remove_workerID and 'workerId' in L.columns:\n",
    "        L = L.drop('workerId',axis=1)\n",
    "        \n",
    "    if eventType in ['block','endTrial','chatMessage']:\n",
    "        L['practice'] = L.trialNum == 'practice'\n",
    "        L.trialNum = pd.to_numeric(L['trialNum'], errors='coerce')\n",
    "\n",
    "    ## save out group dataframe to csv dir\n",
    "    out_path = os.path.join(csv_dir,'compabs_{}_{}.csv'.format(event2name[eventType],iterationName))\n",
    "    print('Saving dataframe out to CSV dir at path: {}'.format(out_path))    \n",
    "    L.to_csv(out_path)             \n",
    "\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataframe out to CSV dir at path: /Users/will/compositional-abstractions/results/csv/compabs_exit_pilot_class_test.csv\n"
     ]
    }
   ],
   "source": [
    "## construct dataframe for each datatype\n",
    "dataTypes = coll.distinct('eventType')\n",
    "for thisDataType in dataTypes:\n",
    "    X = construct_tidy_dataframe(eventType=thisDataType, complete_games=complete_games, iterationName=iterationName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a full DF from DB\n",
    "We can read in a full df from the db and do some basic checks to make sure the data look right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF for each event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/will/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "event_types = ['block', 'chat','exit','trial']\n",
    "iterationList = ['pilot0','pilot1','pilot2','pilot3','pilot4', 'pilot4b']\n",
    "df_dict = {}\n",
    "for event in event_types:\n",
    "    event_dict = {}\n",
    "    df_name = 'df_'+ event\n",
    "    for iteration in iterationList :\n",
    "        df_temp = 'df_'+event+\"_\"+iteration # the name for the dataframe\n",
    "        file_name = '../results/csv/compabs_{}_{}.csv'.format(event,iteration)\n",
    "        event_dict[df_temp] = pd.read_csv(file_name)\n",
    "    df_dict[df_name] = pd.concat(event_dict.values(), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = df_dict['df_block']\n",
    "df_chat = df_dict['df_chat']\n",
    "df_exit = df_dict['df_exit']\n",
    "df_trial = df_dict['df_trial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for char and word counts\n",
    "df_chat['word_count'] = df_chat['content'].str.split(' ').str.len()\n",
    "df_chat['char_count'] = df_chat['content'].str.len()\n",
    "df_chat[\"timeElapsedInTurn\"] = pd.to_numeric(df_chat['timeElapsedInTurn'])\n",
    "\n",
    "# add to trial df\n",
    "trial_sums = df_chat[['gameid','trialNum','word_count','char_count']].groupby(['gameid','trialNum']).sum().reset_index()\n",
    "df_trial = df_trial.merge(trial_sums, how='outer',on=['gameid','trialNum'])\n",
    "\n",
    "# message counts\n",
    "trial_sums = df_chat[['gameid','trialNum','word_count','char_count']].groupby(['gameid','trialNum']).sum().reset_index()\n",
    "df_trial = df_trial.merge(trial_sums, how='outer',on=['gameid','trialNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block['w'] = df_block['width']\n",
    "df_block['h'] = df_block['height']\n",
    "df_block[\"timeElapsedInTurn\"] = pd.to_numeric(df_block['timeElapsedInTurn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrangle timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest time that a block was placed in each trial.\n",
    "# This signifies the end of a trial (i.e. upper bound on total trial time)\n",
    "df_trial_time = df_block[~df_block.practice].groupby(['gameid','trialNum'])\\\n",
    "                ['timeElapsedInTrial'].max().reset_index()\n",
    "df_trial_time = df_trial_time.rename(columns = {'timeElapsedInTrial':'time_final_block'})\n",
    "\n",
    "# Grab the duration of the Architect's turn\n",
    "chat_times = df_chat[~df_chat.practice].groupby(['gameid','trialNum'])\\\n",
    "['timeElapsedInTurn'].sum().reset_index()\n",
    "\n",
    "# total time to place blocks (by taking summing maximum block placement time for each turn)\n",
    "total_block_times = df_block[(~df_block.practice)].groupby(['gameid','trialNum','turnNum'])\\\n",
    "                    ['timeElapsedInTurn'].max()\\\n",
    "                    .groupby(['gameid','trialNum']).sum().reset_index()\n",
    "\n",
    "# # time from trial_start to final block placed in turn, summed across the trial\n",
    "df_trial_time['total_block_duration'] = total_block_times['timeElapsedInTurn'] \n",
    "\n",
    "# time from trial_start to chat message sent, summed across the trial\n",
    "df_trial_time['total_chat_duration'] = chat_times['timeElapsedInTurn']\n",
    "\n",
    "# # sum of block placement and chat time in a trial\n",
    "df_trial_time['total_duration'] = df_trial_time['total_chat_duration'] + df_trial_time['total_block_duration']\n",
    "\n",
    "# # differnce between final block placement time and total chat plus block placement time.\n",
    "# # i.e. the total extra time in a trial, mainly consisting of time to press done button\n",
    "df_trial_time['diff'] = df_trial_time['time_final_block'] - df_trial_time['total_duration']\n",
    "\n",
    "# in addition, add a total_turn_duration- \n",
    "#  the time between this turn start and the next turn start (or final block placement)\n",
    "#  as an estimate for when the Done button is pressed.\n",
    "# didn't have the data for this in pilot3- we are now saving turnStartTime\n",
    "\n",
    "# add timing info to trial df\n",
    "df_trial = df_trial.merge(df_trial_time, how='left', on=['gameid','trialNum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flag participants with negative timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_timing_data = df_block[df_block['timeElapsedInTurn']<0].gameid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block['flagged'] = df_block.gameid.isin(bad_timing_data)\n",
    "df_chat['flagged']  = df_chat.gameid.isin(bad_timing_data)\n",
    "df_exit['flagged']  = df_exit.gameid.isin(bad_timing_data)\n",
    "df_trial['flagged'] = df_trial.gameid.isin(bad_timing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dataframes from each eventType\n",
    "df_block.to_csv('../results/csv/df_block.csv', header=True, index=False) \n",
    "df_chat.to_csv('../results/csv/df_chat.csv', header=True, index=False) \n",
    "df_exit.to_csv('../results/csv/df_exit.csv', header=True, index=False) \n",
    "df_trial.to_csv('../results/csv/df_trial.csv', header=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pilot0', 'pilot1', 'pilot2', 'pilot3', 'pilot4', 'pilot4b'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chat.iterationName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get count of dyads that have 75 on 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dyads achieving 75% Accuracy on 75% of trials: 49\n"
     ]
    }
   ],
   "source": [
    "df_trial = pd.read_csv('df_trial.csv')\n",
    "df75 = pd.DataFrame(df_trial.groupby(['gameid', 'trialNum'])['trialScore'].sum()>75).groupby(['gameid']).sum()\n",
    "df75['trials'] = df75['trialScore']\n",
    "\n",
    "#delete rows that aren't greater than 75% in 9 trials\n",
    "df75 = df75[df75['trials']>=9]\n",
    "\n",
    "print(\"Total dyads achieving 75% Accuracy on 75% of trials:\",len(df75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
