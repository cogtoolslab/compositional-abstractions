{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42cf2824",
   "metadata": {},
   "source": [
    "# Tutorial notebook: copied here for reference\n",
    "# Learning to communicate about shared procedural abstractions\n",
    "## Notebook 3: Forming conventions to talk about shared abstractions\n",
    "\n",
    "Preparation of this notebook was led by [Robert Hawkins](https://rdhawkins.com/).\n",
    "\n",
    "These results were originally reported in: \n",
    "[McCarthy*, W., Hawkins*, R., Wang, H., Holdaway, C., and Fan, J. (2021). Learning to communicate about shared procedural abstractions. Proceedings of the 43rd Annual Meeting of the Cognitive Science Society.](https://cogtoolslab.github.io/pdf/mccarthy_cogsci_2021b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50d907",
   "metadata": {},
   "source": [
    "### *NOTE: THIS NOTEBOOK SERVES AS THE \"INSTRUCTOR\" VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df40cc",
   "metadata": {},
   "source": [
    "In the previous notebook we studied the problem of **concept learning**. We wanted to know how participants might have 'chunked' blocks together into larger abstractions as they proceeded through the twelve trials of the task. We worked through a model of abstraction as library learning, where people build up a collection of program fragments corresponding to possible concepts (literally 'building blocks', if you like). For each trial, we could then use the current library to propose possible programs that express the scene in different ways. \n",
    "\n",
    "In this notebook, we extend our model to address the **communication** problem. That is, given a tower scene and a set of concepts, what linguistic instructions should the Architect give to the Builder that will allow them to successfully reconstruct that scene? And given a set of instructions and a potentially different set of concepts, what actions should the Builder actually perform?\n",
    "\n",
    "We approach this problem in a **probabilistic modeling** framework based on [Hawkins et al. (2023)](https://cocosci.princeton.edu/papers/hawkinspartners.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebdb01",
   "metadata": {},
   "source": [
    "This notebook is divided into 4 sections that incrementally build up to the full model.\n",
    "\n",
    "**Section 1** begins by implementing the core notion of a *mental lexicon* -- a mapping from words to concepts.\n",
    "\n",
    "**Section 2** implements basic Architect and Builder agents that make decisions based on fixed lexicons.  \n",
    "\n",
    "**Section 3** equips these agents with the ability to *learn* and update their *beliefs* about the lexicon over time.\n",
    "\n",
    "**Section 4** finally reaches the core theoretical question of why speakers prefer one level of abstraction over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b1581",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import choice\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classes for our model\n",
    "sys.path.append(\"../model/convention_formation/\")\n",
    "from distribution import *\n",
    "from lexicon import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76276935",
   "metadata": {},
   "source": [
    "## Section 1: Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946d9fc",
   "metadata": {},
   "source": [
    "### Representing lexicons\n",
    "\n",
    "The basic building block of our model is an agent's **mental lexicon**, a particular set of correspondances between concepts and words. To define an agent model, we must first define the mental lexicon. \n",
    "\n",
    "We have created a class for this purpose called `BlockLexicon()` which manages the mapping between program primitives (as defined in **[the previous section](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/notebooks/ca_programs.ipynb)**) to words and phrases. If you're interested in digging into the nitty-gritty details, we've put this class in a helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/lexicon.py)__).\n",
    "\n",
    "For our purposes, though, we don't need to know exactly what's going on under the hood. Let's take the lexicon class out for a drive. We need to initialize it with two parameters, essentially corresponding to the base entities on each side of the mapping we want to define.\n",
    "\n",
    "(1) `dsl`: a list of concepts in the domain-specific language (DSL) that might be expressed as words\n",
    "\n",
    "(2) `lexemes`: a list of words that are available to bind to new concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c9c61",
   "metadata": {},
   "source": [
    "To build up our intuitions, we're going to work with a very simplified example lexicon, where the set of concepts is the library on trial 10 of the trial sequence presented to one of our pairs of human participants (retrieved from the file we saved out in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the DSL primitives accessible at trial 10 for the first dyad in our empirical data\n",
    "empirical_data = pd.read_json('../data/model/programs_for_you/programs_ppt_1.json')\n",
    "dsl = empirical_data.iloc[10]['dsl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2269d",
   "metadata": {},
   "source": [
    "We'll also define a list of lexemes, a bunch of nonsense words that won't start out meaning anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of meaningless placeholder words available to be bound to meanings\n",
    "lexemes = ['blah', 'blab', 'bloop', 'bleep', 'floop'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcadb03",
   "metadata": {},
   "source": [
    "We'll use these two ingredients to construct a `BlockLexicon` (defined [here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/lexicon.py)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6360ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = BlockLexicon(dsl, lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861d6e",
   "metadata": {},
   "source": [
    "The lexicon object `l` that we've created comes with a lot of handy built-in properties, including language for talking about various primitive actions of the dsl (like placing blocks). We can check these out by calling some of its functions. For example, the `dsl_to_language` method looks up which word to use for any element of the `dsl`. \n",
    "\n",
    "> _Note_: Function that belong to a Python class are typically called \"methods\" (e.g., `dsl_to_language` is a method of the `BlockLexicon` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a04630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first element in the list of DSL primitives (h) is already bound to the language \"place a horizontal block.\"')\n",
    "print(dsl[0], '->', l.dsl_to_language(dsl[0]))\n",
    "print('')\n",
    "print('Here are some other examples.')\n",
    "print(dsl[10], '->', l.dsl_to_language(dsl[10]))\n",
    "print(dsl[-1], '->', l.dsl_to_language(dsl[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d3ae1",
   "metadata": {},
   "source": [
    "We can also go in the other direction using the `language_to_dsl` function, converting from a linguistic expression to a corresponding primitive in the DSL.\n",
    "We went ahead and 'baked in' correspondences for the basic elements of our DSL, because we're assuming these meanings are pretty much deterministic; there's not much wiggle room about what 'place a horizontal block' or 'move to the left by 8' means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a796976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('place a horizontal block. ->', l.language_to_dsl('place a horizontal block.'))\n",
    "print('move to the left by 8 ->', l.language_to_dsl('move to the left by 8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39290a",
   "metadata": {},
   "source": [
    "> _Note_: If we wanted a more realistic model that worked on generic natural language, e.g. if we wanted to pair our agent with a real user writing in a chat box, we would want something more robust that doesn't require exact string matching. We might use a simple algorithm to find the nearest string in the lexicon, or we might use a fancier model operating over utterance embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817c7d3",
   "metadata": {},
   "source": [
    "If you play around with the `BlockLexicon()` class, you may notice a subtlety. If we pass in an unfamiliar utterance that isn't found in the list of lexemes we provided (e.g., `place a blah` shown below), it will return one of the concepts that doesn't already have a word assigned. We can think of this like an agent randomly 'guessing' rather than throwing an error. Likewise, if we pass in a concept that is not in the DSL, it will return a randomly sampled element from the list of lexemes. (If you re-run the cell below multiple times, you'll notice that the output will be different each time)\n",
    "\n",
    "> _Note_: A more principled but complicated approach to this technical problem is to define a 'null meaning' and 'null utterance' that can be returned in these cases instead of a random guess. These 'null' objects can roughly interpreted be interpreted as \"do nothing\" and \"say nothing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49892a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Passing in \"place a blah\"...')\n",
    "print('place a blah. ->', l.language_to_dsl('place a blah.'))\n",
    "print('')\n",
    "print('Passing in \"chunk_R\"...')\n",
    "print('chunk_R ->', l.dsl_to_language('chunk_R'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb8d51",
   "metadata": {},
   "source": [
    "### Representing beliefs about lexicons\n",
    "\n",
    "The lexicon `l` we've been playing with so far is a deterministic data structure; it's just a single mapping. \n",
    "\n",
    "In a probabilistic model, however, we want to be able to talk in a mathematically rigorous way about an agent's subjective **beliefs** about a mapping. In other words, we want to be able to define a **probability distribution** over all possible lexicons $\\mathcal{L}$. This will provide a precise definition for an agent's uncertainty about the lexicon in their partner's head, i.e. they assume their partner must be using one of these mappings $\\mathcal{L}^*$, but do not know ahead of time exactly what it is.\n",
    "\n",
    "We're going to introduce another custom class we've written called a `Distribution()`, which maintains the probabilities assigned to each possible lexicon. (Again, for nitty-gritty details, see the helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/distribution.py)__). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161bb0b",
   "metadata": {},
   "source": [
    "We'll define a **prior** distribution (the agent's initial beliefs) as a uniform distribution over all possible ways of binding elements in the DSL to the lexemes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74708fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initializing a LexiconPrior from a specific DSL and a list of lexemes\n",
    "prior = LexiconPrior(dsl, lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deedfd9",
   "metadata": {},
   "source": [
    "We've also created some handy helper functions to work with distributions. For example, `prior.support()` returns the support of the distribution `prior`, i.e. the list of values that it is defined over. So we can print out the first lexicon in the support as an example of what lexicons look like. In this case, there are 24 lexicons in the support of the prior because there are exactly 24 ways of binding the four non-primitives in the DSL (e.g., \"chunk_L\") to unique lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lexicon = prior.support()[0]\n",
    "print('example element:', example_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab837ee",
   "metadata": {},
   "source": [
    "We can also use `prior.score(l)` to return the probability of a lexicon `l` under the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"P(^ that lexicon) = 1/{len(prior.support())} = {prior.score(example_lexicon)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864d0c9",
   "metadata": {},
   "source": [
    "There are a few other things you can do with a distribution object. For example, you can sample an element from the distribution with `prior.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e8b90",
   "metadata": {},
   "source": [
    "And we can use the `marginalize` function to collapse down to the mapping for a single dsl or language element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('possible utterances for chunk_L : ', \n",
    "      prior.marginalize(lambda l : l.dsl_to_language('chunk_L')))\n",
    "\n",
    "print('possible concepts for \"place a blah\" : ', \n",
    "      prior.marginalize(lambda l : l.language_to_dsl('place a blah.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79b923",
   "metadata": {},
   "source": [
    "Here, we can verify that the novel element 'chunk_L' in the DSL is initially equally likely to be mapped to any of the novel lexemes: $p = 1/|U|$. The equal probabilities for each expression tell us that the Agents think each expression is an equally good (or bad) translation of \"chunk_L\".\n",
    "\n",
    "> _Note:_  A uniform prior makes sense for this artificial language (where \"bleeps\" and \"bloops\" are used to refer to abstractions). Real people likely have strong priors about what words will mean (we can make quite a lot of sense of \"build an L\" before any shared experience, even if there's some remaining uncertainty about properties of the L like its size and width, etc). But a uniform prior helps us understand the dynamics of coordination in the most extreme case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda5be5",
   "metadata": {},
   "source": [
    "## Section 2: Simulating Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f071d0a",
   "metadata": {},
   "source": [
    "Now we have a way of representing an agent's beliefs over lexicons, we can define an Architect and Builder.\n",
    "\n",
    "Both maintain their own belief distribution about the *other agent's* lexicon.\n",
    "\n",
    "The **Architect** agent makes choices about what to say based by imaginging how the Builder will interpret them.  \n",
    "\n",
    "The **Builder** takes actions based on its beliefs about what the Architect would say in different situations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedAgent() :\n",
    "    def __init__(self, role, trial) :\n",
    "        '''\n",
    "        Args: \n",
    "           * role: string giving agent's role in the task ('architect' or 'builder')\n",
    "           * trial: dictionary of meta-data about the current trial \n",
    "        '''\n",
    "\n",
    "        # initialize beliefs to a uniform prior over possible lexicons, as above\n",
    "        self.beliefs = LexiconPrior(trial['dsl'], lexemes)\n",
    "\n",
    "        # set other useful properties\n",
    "        self.role = role\n",
    "        self.actions = trial['dsl']\n",
    "        self.utterances = self.beliefs.sample().utterances\n",
    "        \n",
    "    def act(self, observation) :\n",
    "        '''\n",
    "        produce an action based on role and current beliefs\n",
    "        '''\n",
    "        if self.role == 'architect' :\n",
    "            # Architect is going to build up a distribution over utterances to say\n",
    "            utt_dist = self.beliefs.marginalize(lambda l : l.dsl_to_language(observation))\n",
    "            return utt_dist.sample()\n",
    "\n",
    "        if self.role == 'builder' :\n",
    "            # get P(a | utt) by marginalizing over lexicons \n",
    "            action_dist = self.beliefs.marginalize(lambda l : l.language_to_dsl(observation))\n",
    "            return action_dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7789016",
   "metadata": {},
   "source": [
    "Because our focus is on modeling abstractions learned during the task, we assume that Architect Agents and Builder Agents can unambiguously communicate about the base DSL-- moving left and right and placing individual blocks. I.e. when an Architect wants a Builder to place a block they will always say \"place a horizontal block\", and the Builder will correctly interpret this utterance.\n",
    "\n",
    "The only thing that varies across different lexicons is the words used for *learned* program fragments. In practice (for this example) only five of these were learned across all participants' trial sequences. The set of possible lexicons is therefore fully defined by the set of possible mappings from these fragments to the `lexemes` defined above.\n",
    "\n",
    "For the built-in primitives like the horizontal block, this should be a deterministic choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_trial = empirical_data.iloc[0].to_dict()\n",
    "architect = FixedAgent('architect', first_trial)\n",
    "print('architect utterance: `h` -> ', architect.act('h'))\n",
    "\n",
    "builder = FixedAgent('builder', first_trial)\n",
    "print('builder choice: \"place a horizontal block\" -> ', builder.act('place a horizontal block.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caeba11",
   "metadata": {},
   "source": [
    "### Running simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607ec60",
   "metadata": {},
   "source": [
    "Now we have our agents, let's see what they do when they proceed through the same sequence of 12 trials as one human dyad from our behavioral experiment.\n",
    "\n",
    "On each trial, the (Fixed) Architect will see a tower scene and then write a program in their DSL that could re-generate the scene. Then for each `step` of the program, we call `architect.act(step)` to get a corresponding linguistic instruction (`utt`, for utterance). We then pass that instruction (`utt`) to the builder agent who will select an action in their own DSL using the function `builder.act(utt)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(trial_info) :\n",
    "    output = SimulationOutput()\n",
    "    for i, trial in trial_info.iterrows() :\n",
    "        # Initialize agents using current trial metadata\n",
    "        architect = FixedAgent('architect', trial)\n",
    "        builder = FixedAgent('builder', trial)\n",
    "\n",
    "        # if there are multiple program representations, randomly \n",
    "        # select which one to comunicate (we will return to this)\n",
    "        target_program = choice(list(trial['programs_with_length'].keys()))\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        # produce an utterance from architect and response from builder\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            output.save(step, utt, response) \n",
    "\n",
    "        # flush output buffer and prepare for next trial\n",
    "        output.flush(trial, target_program)\n",
    "    return output.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1956ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can call the run_simulation function above to simulate the utterances and actions that would be generated \n",
    "# in a 12-trial interaction between the Architect and Builder. Note that if you run it multiple times, you'll get \n",
    "# different simulated outcomes each time. \n",
    "run_0 = run_simulation(empirical_data)\n",
    "display(run_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first trial\n",
    "print('target program: \\n', run_0.query('trial==1').loc[0,'target_program'])\n",
    "run_0.query('trial==1')[['utterance','intention','response','acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94451d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the final trial\n",
    "print('target program: \\n', run_0.query('trial==12').loc[0,'target_program'])\n",
    "run_0.query('trial==12')[['utterance','intention','response','acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d0bef",
   "metadata": {},
   "source": [
    "Notice that the final trial may use some chunks, but their correspondence to utterances have not changed between the first and final trials. That is because these are \"fixed\" agents who do not yet have a way of updating their lexicons!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582d84d",
   "metadata": {},
   "source": [
    "### <span style=\"color: orange\"> Exercise: explore how accuracy changes </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7875b5",
   "metadata": {},
   "source": [
    "If they do use chunks, you might notice that the accuracy gets worse over time because our agents aren't actually *learning* -- they're continuing to use their initial uniform priors. We can take a look at this by analyzing how accuracy (`acc`) changes across trials in the `run_0` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da395c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE BLOCK FOR EXERCISE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77280cda",
   "metadata": {},
   "source": [
    "## Section 3: Simulating learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507bd4f",
   "metadata": {},
   "source": [
    "In a probabilistic framework, learning is equivalent to updating one's beliefs given new observations. We thus need to extend our `FixedAgent` class from Section 2 with a `update_beliefs()` function, using Bayes' rule to turn their prior into a posterior, $P(\\mathcal{L} | o)$, using the outcomes `o` on the previous trials. Here's Bayes' rule, which gives us a blueprint to handle this:\n",
    "\n",
    "$$P(\\mathcal{L} | o) = \\frac{P(o | \\mathcal{L})P(\\mathcal{L})}{\\sum_{\\mathcal{L}} P(o | \\mathcal{L}) P(\\mathcal{L})}$$\n",
    "\n",
    "We already have the prior term $P(\\mathcal{L})$, but we're missing the likelihood term $P(o | \\mathcal{L})$ which scores how likely a given utterance or action on the previous trials would be under different lexicons $\\mathcal{L}$. \n",
    "\n",
    "Following Hawkins et al. (2023), we use a likelihood where each agent reasons about a (simplified) mental model of the other agent, and asks how likely the other agent would be to make a given choice if they had a given lexicon in their head. Specifically, the architect tries to design utterances to maximize the probability of success assuming a literal builder $B_0(a | u)$, which chooses among actions $a$ that are literally consistent with the utterance $u$ they hear. Meanwhile, the builder tries to pick actions assuming the architect $A_0(u | a^*)$ is choosing utterances $u$ that are literally consistent with the target step $a^*$ they are trying to convey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7daa0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAgent(FixedAgent) :\n",
    "    def __init__(self, role, curr_trial, previous_trials) :\n",
    "        super().__init__(role, curr_trial)\n",
    "        combined_primitives = (set().union(curr_trial['dsl'], *previous_trials['dsl']) \n",
    "                                if not previous_trials.empty else self.actions)\n",
    "\n",
    "        # Initialize beliefs to uniform prior and then compute \n",
    "        self.prior = LexiconPrior(combined_primitives, lexemes)\n",
    "        self.update_beliefs(previous_trials)\n",
    "\n",
    "    def B0(self, utterance, lexicon) :\n",
    "        '''\n",
    "        simple builder agent that has equal probability\n",
    "        of building anything that's literally consistent with the utterance\n",
    "        '''\n",
    "        builder_dist = EmptyDistribution()\n",
    "        for action in self.actions :\n",
    "            builder_dist.update({action : 1 if action == lexicon.language_to_dsl(utterance) else 0.01})\n",
    "        builder_dist.renormalize()\n",
    "        return builder_dist\n",
    "        \n",
    "    def A0(self, intention, lexicon) :\n",
    "        '''\n",
    "        simple architect agent that has equal probability\n",
    "        of saying anything that's literally consistent with the intention\n",
    "        '''\n",
    "        architect_dist = EmptyDistribution()\n",
    "        for utt in self.utterances :\n",
    "            architect_dist.update({utt : 1 if utt == lexicon.dsl_to_language(intention) else 0.01})\n",
    "        architect_dist.renormalize()\n",
    "        return architect_dist\n",
    "\n",
    "    def update_beliefs(self, previous_trial_df) :\n",
    "        '''\n",
    "        run bayes rule given observations in previous trials\n",
    "        note that we run the calculation in log space because it's more numerically stable\n",
    "        '''\n",
    "        posterior = EmptyDistribution()\n",
    "        posterior.to_logspace()\n",
    "\n",
    "        # we're manually doing the calculation in Bayes Rule\n",
    "        # P(l | obs) \\propto P(l) * \\prod_{o \\in obs} P(o | l)\n",
    "        # ==> log P(l|obs) \\propto log P(l) + \\sum_{o \\in obs} log P(o | l)\n",
    "        for lexicon in self.prior.support() :\n",
    "            # calculate the likelihood of the previous data under each lexicon, \n",
    "            likelihood_term = 0\n",
    "            for i, step in previous_trial_df.iterrows() :\n",
    "                if self.role == 'architect' :\n",
    "                    likelihood_term += np.log(self.B0(step.utterance, lexicon).score(step.response))\n",
    "                elif self.role == 'builder' :\n",
    "                    likelihood_term += np.log(self.A0(step.intention, lexicon).score(step.utterance))\n",
    "            \n",
    "            # weight by the prior probability of that lexicon\n",
    "            prior_term = np.log(self.prior.score(lexicon))\n",
    "            posterior.update({lexicon : prior_term + likelihood_term})\n",
    "\n",
    "        # Renormalize (this is the \\propto part of Bayes Rule, the denominator)\n",
    "        posterior.renormalize()\n",
    "        posterior.from_logspace()\n",
    "        self.beliefs = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb5113",
   "metadata": {},
   "source": [
    "Take a moment to inspect the `LearningAgent` class definition above. In particular, pay special attention to the `update_beliefs()` function which performs the heavy lifting to actually update each agent's beliefs based on what they have observed so far (`previous_trial_df`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f35f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_learning_simulation(trial_info, verbose = False) :\n",
    "    output = SimulationOutput()\n",
    "    for i, current_trial in trial_info.iterrows() :\n",
    "        clear_output(wait=True)\n",
    "        print(f'trial {i}/12')\n",
    "        \n",
    "        # construct agents with updated beliefs up to this point\n",
    "        previous_trials = output.get_df()\n",
    "        architect = LearningAgent('architect', current_trial, previous_trials)\n",
    "        builder = LearningAgent('builder', current_trial, previous_trials) \n",
    "\n",
    "        # random program selected from the options\n",
    "        target_program = choice(list(current_trial['programs_with_length'].keys()))\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            output.save(step, utt, response) \n",
    "        \n",
    "        output.flush(current_trial, target_program)\n",
    "    return output.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c722315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_run_0 = run_learning_simulation(empirical_data, verbose = True)\n",
    "learning_run_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8ee6b",
   "metadata": {},
   "source": [
    "## Section 4: Choosing programs.\n",
    "\n",
    "In principle, words allow us to communicate arbitrarily complex concepts, and our mental representation of linguistic meaning is flexible enough to be updated over time. To the extent that both Architects and Builders are learning to 'chunk' blocks over time, each  program fragment) could, in principle, be assigned a new word or phrase, and conveyed directly through the Architects' instructions. Well, almost. There is no guarantee that the words the Architect chooses to pick out a new concept would invoke the same concept in someone else. There is actually uncertainty about how the Builder will interpret the Architects' instructions and this, we suggest, might change what the Architect chooses to say.\n",
    "\n",
    "Our hypothesis is that Architects trade-off communicative *efficiency* with communicative *effectiveness*. While they generally want to say things concisely, if there is too much uncertainty about how the Builder will interpret their words, they will choose a less ambiguous (if more verbose) way of expressing the same information. Concretely, if the Architect wanted to say \"build an L\" but they thought that the Builder wouldn't get what \"L\" meant, they might spell out the steps to make an L-shaped tower instead.\n",
    "\n",
    "Wow, this is great! We can see our agents are updating their beliefs about the lexicon over time and able to get somewhat more accurate as they coordinate. But one of the most interesting things about our empirical data is that speakers seem to be strategically choosing which representation of the tower to convey -- our hypothesis is that the reason participants reduce the length of their utterances over time is that even when new library chunks come online, architects don't always try to refer to them right away. They aren't confident enough that their partner will understand, as the block-level descriptions are much safer. However, the block-level descriptions are also much *costlier* in terms of time and effect because they have to laboriously describe one action at a time. \n",
    "\n",
    "So far, we just used a placeholder for how the speaker picks which representation to communication: they just randomly pick from the list of candidates, slightly preferring shorter programs. However, there are other considerations that ought to go into this decision, namely the estimated likelihood that the listener will do the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a705268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class StrategicArchitect(LearningAgent) :\n",
    "    def __init__(self, curr_trial, previous_trials) :\n",
    "        super().__init__('architect', curr_trial, previous_trials) \n",
    "        self.alpha = 2\n",
    "        self.beta = .3\n",
    "\n",
    "    def expected_inf(self, utterance, intention) :\n",
    "        '''\n",
    "        computes the expected utility of transmitting the given intention with utterance\n",
    "        accounting for uncertainty of whether utterance will work\n",
    "        '''\n",
    "            # calculate expected inf(u) = \\sum_a*\\in A* \\sum_L P(L) * ln P_B(a* | u, L) \n",
    "        return sum([\n",
    "            self.beliefs.score(l) * np.log(self.B0(utterance, l).score(intention))\n",
    "            for l in self.beliefs.support()\n",
    "        ])\n",
    "        \n",
    "    def speak(self, possible_programs) :\n",
    "        '''\n",
    "        produce an action based on role and current beliefs\n",
    "        '''\n",
    "        # Architect is going to build up a distribution over utterances to say\n",
    "        # Architect selects which program representation to comunicate proportional to informativity and length\n",
    "        p_utils = []\n",
    "        for target_program in possible_programs: \n",
    "            step_utils = []\n",
    "            for step in target_program.split(' ') :\n",
    "                utt_utils = np.array([self.expected_inf(utt, step) for utt in self.utterances])\n",
    "                step_utils.append(sum(utt_utils * softmax(self.alpha * utt_utils)))\n",
    "            p_utils.append(   \n",
    "                (1 - self.beta) * np.mean(step_utils) \n",
    "              - (    self.beta) * len(target_program.split(' '))\n",
    "            )\n",
    "            print(p_utils)\n",
    "        # sample a program\n",
    "        chosen_p = choice(a = possible_programs, p = softmax(self.alpha * np.array(p_utils)))\n",
    "        # sample utterances for that program\n",
    "        return chosen_p, [self.act(step) for step in chosen_p.split(' ')]\n",
    "\n",
    "def run_strategic_simulation(empirical_data) :\n",
    "    output = SimulationOutput()\n",
    "    for i, current_trial in empirical_data.iterrows() :\n",
    "        previous_trials = output.get_df()\n",
    "        architect = StrategicArchitect(current_trial, previous_trials)\n",
    "        builder = LearningAgent('builder', current_trial, previous_trials)\n",
    "\n",
    "        # Jointly pick a program to communicate, and what to say\n",
    "        possible_programs = list(current_trial['programs_with_length'].keys())\n",
    "        chosen_program, utt_seq = architect.speak(possible_programs)\n",
    "        print(f'trial: {i}, chosen program: {chosen_program}')\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        for intention, utt in zip(chosen_program.split(' '), utt_seq) :\n",
    "            response = builder.act(utt)\n",
    "            output.save(intention, utt, response)\n",
    "            \n",
    "        output.flush(current_trial, chosen_program)\n",
    "    return output.get_df()\n",
    "\n",
    "print(run_strategic_simulation(empirical_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2148685",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "\n",
    "Congratulations! You've finished the tutorial!\n",
    "\n",
    "We hope you found it fun and informative. :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7499e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
