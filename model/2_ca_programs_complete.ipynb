{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5a7b5c2",
   "metadata": {},
   "source": [
    "# Tutorial notebook: copied here for reference\n",
    "# Learning to communicate about shared procedural abstractions\n",
    "## Notebook 2: Learning part concepts with program abstraction\n",
    "\n",
    "This notebook was prepared by [Will McCarthy](https://wpmccarthy.com/).\n",
    "\n",
    "These results were originally reported in: \n",
    "[McCarthy*, W., Hawkins*, R., Wang, H., Holdaway, C., and Fan, J. (2021). Learning to communicate about shared procedural abstractions. Proceedings of the 43rd Annual Meeting of the Cognitive Science Society.](https://cogtoolslab.github.io/pdf/mccarthy_cogsci_2021b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3786bf3",
   "metadata": {},
   "source": [
    "### *NOTE: THIS NOTEBOOK SERVES AS THE \"INSTRUCTOR\" VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6de21",
   "metadata": {},
   "source": [
    "In the previous section, we saw that *Architects* used increasingly concise language to describe the scenes they were viewing. In particular, we saw that they started to use words that referred to increasingly complex entities, moving from instructions about individual blocks to entire towers.\n",
    "\n",
    "Here we try to explain this trend through the lens of *abstraction*. We hypothesize that, as people are exposed to scenes that have elements in common (i.e. towers), they acquire a vocabulary of increasingly abstract *part concepts* that they can use represent and talk about each scene more concisely. We formalize this idea using *programmatic representations* of scenes, which support an amodal kind of abstraction, in the form of *program fragments*. In this notebook we explore a mechanism for learning program fragments as a model for the learning of part concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73607a",
   "metadata": {},
   "source": [
    "This section is divided into three sections.  \n",
    "**Section 1** explains the programmatic representations we will work with.  \n",
    "**Section 2** covers the aquisition of part concepts over trials.  \n",
    "**Section 3** covers the refactoring of scene programs into more concise programs involving these part concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e3ea2",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "sys.path.append(\"../model/lib_learning/\")\n",
    "\n",
    "from program import *\n",
    "import utilities\n",
    "import render\n",
    "from parsePrograms import *\n",
    "\n",
    "from towerPrimitives import primitives\n",
    "from makeTowerTasks import *\n",
    "from grammar import *\n",
    "from fragmentGrammar import *\n",
    "from gen_seq import *\n",
    "from enumeration import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import block rendering\n",
    "sys.path.append(\"../model/block_utils/\")\n",
    "\n",
    "from block_utils import render_program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62aa708",
   "metadata": {},
   "source": [
    "## Section 1: Representing block towers as programs\n",
    "\n",
    "We begin with the key idea that it is possible to represent any block tower in our experiment as a computer-graphics program containing instructions for placing (or \"drawing\") blocks in specific locations in some order. You can think of this program as containing \"procedural knowledge\" about how to build a block tower. \n",
    "\n",
    "In this project we want to explore how people learn *abstract* procedural knowledge. We therefore assume that people are already familiar with a set of primitive concepts (e.g., what a block is) and ask how they acquire more complex concepts (e.g., multi-block configurations) that build on that foundation. \n",
    "\n",
    "In the program synthesis literature, the set (or \"library\") of concepts that can be invoked by a program to execute a task in some application domain is also called a *domain specific language* (DSL). We will call the initial set of primitive concepts our \"base DSL\" and the process of building more complex cconcepts \"library learning.\"\n",
    "\n",
    "Our *base DSL* (adapted from a similar task in [Dreamcoder](https://arxiv.org/abs/2006.08381)), contains the following primitives:\n",
    "- **h**: place horizontal domino\n",
    "- **v**: place vertical domino\n",
    "- **l_x**: move left x places,  where x in {1,2,3,4,5,6,7,8,9,10,11,12}\n",
    "- **r_x**: move right x places, where x in {1,2,3,4,5,6,7,8,9,10,11,12}\n",
    "\n",
    "Later in the notebook, we will extend this base DSL with higher level abstractions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54ce4d",
   "metadata": {},
   "source": [
    "We manually define a unique program for each scene. For example, the scene with the C-tower on the left and L-tower on the right is represented as:  \n",
    "    `h l_1 v v r_1 h r_12 h l_4 h l_1 v v`\n",
    "    \n",
    "We've built a little renderer for programs involving these tokens so you can see how they ground out as block towers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bef77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place a horizontal block, move right 8 spaces, place a vertical block\n",
    "render_program(\"h r_8 v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c2bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# render a program for one of our tower scenes\n",
    "render_program(\"h l_1 v v r_1 h r_12 h l_4 h l_1 v v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9709803",
   "metadata": {},
   "source": [
    "Here is a dictionary of programs for all of the possible scenes in our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_tower_programs = {\"CL\" :\"h l_1 v v r_1 h r_12 h l_4 h l_1 v v\",\n",
    "                         \"CPi\": \"h l_1 v v r_1 h r_6 v r_6 v l_5 h r_4 h\",\n",
    "                         \"PiC\": \"v r_6 v l_5 h r_4 h r_7 h l_1 v v r_1 h\",\n",
    "                         \"LPi\": \"h l_4 h l_1 v v r_9 v r_6 v l_5 h r_4 h\",\n",
    "                         \"LC\": \"h l_4 h l_1 v v r_12 h l_1 v v r_1 h\",\n",
    "                         \"PiL\": \"v r_6 v l_5 h r_4 h r_9 h l_4 h l_1 v v\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e2dec",
   "metadata": {},
   "source": [
    "## <span style=\"color: orange\"> Exercise: familiarize yourself with tower programs </span>\n",
    "\n",
    "To get a feel for how these programs work, try the following exercises. Also, just have a place placing blocks in different locations.\n",
    "\n",
    "1. Render each of the towers defined in the dictionary above.\n",
    "2. Render each of the towers (C, Pi, and L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7dd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render a scene program\n",
    "render_program(manual_tower_programs[\"CPi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render a tower\n",
    "render_program('h l_1 v v r_1 h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7094fd",
   "metadata": {},
   "source": [
    "## Section 2: Library learning\n",
    "\n",
    "In this section we formalize abstraction learning as the discovery of *program fragments*. Fragments can be thought of as a functional substring of a program, or subroutine. By design, the scenes we used in our experiment have several subparts that we expect will be particularly useful-- namely the 3 towers that they are constructed from.\n",
    "\n",
    "**By augmenting the base DSL with additional tokens that correspond to program fragments, we can create news DSL that can be used to express scenes more efficiently (i.e. with fewer tokens).** For example, we represent the \"CL\" scene in our base DSL as `\"h l_1 v v r_1 h r_12 h l_4 h l_1 v v\"`. But say we had a token `\"chunk_C\"` that encapsulated all of the moves required to place the C-shaped tower. Then we could rewrite the scene program as `\"chunk_C r_12 h l_4 h l_1 v v\"` (which uses fewer tokens).\n",
    "\n",
    "This benefit comes at the cost of storing the new abstractions in memory. This trade-off is captured by the abstraction learning algorithm, [Dreamcoder](https://arxiv.org/abs/2006.08381). Dreamcoder adds program fragments to a DSL more or less readily depending on a weighting parameter, *w*. To keep things simple, here we consider a small range of *w*s (1.5, 3.3, 9.6) that produce a reasonable range of *learning rates*. This is important, as we are using Dreamcoder to capture the *change in DSLs* across trials. As the model is exposed to more scenes (programs), fragments are added to the DSL, and scenes are able to be expressed with shorter programs. This is meant to capture the ability of participants to reason about structures larger than individual blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe2d26",
   "metadata": {},
   "source": [
    "### Loading dreamcoder libraries\n",
    "\n",
    "To save time, we have already run library learning using the Dreamcoder algorithm for you, for a few different weight parameter values, for each participant's trial sequence.\n",
    "\n",
    "The output is a series of DSLs-- one for each trial, for each participant, for each weight-- that (may or may not) contain additional program fragments.\n",
    "\n",
    "These DSLs are saved in `./data/model/dsls/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "ws = [1.5, 3.3, 9.6] # values of w we are considering\n",
    "w_index = {1.5 : 0,\n",
    "           3.3 : 2,\n",
    "           9.6: 3} # positional index of w in loaded data\n",
    "trials = range(1,13)\n",
    "ppts = range(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the DSLs learned by dreamcoder\n",
    "\n",
    "data_path = '../data/model/dsls/'\n",
    "\n",
    "dsls = {}\n",
    "trial_seqs = {}\n",
    "\n",
    "for ppt in range(1,50):\n",
    "    \n",
    "    dsls[ppt] = {}\n",
    "    \n",
    "    # read participants' trial sequence\n",
    "    with open(data_path+f\"{ppt}/configs.p\", \"rb\") as config_file:\n",
    "            trial_seqs[ppt] = pickle.load(config_file)\n",
    "    \n",
    "    # read inferred DSLs\n",
    "    for trial in range(1, 13):\n",
    "        with open(data_path+f\"{ppt}/{trial}.p\", \"rb\") as input_file:\n",
    "            dsls[ppt][trial] = pickle.load(input_file)\n",
    "\n",
    "def check_values(value, valid_values, parameter_type):\n",
    "    if value not in valid_values:\n",
    "        raise ValueError(f'{parameter_type} must be one of the following values: {valid_values}.')\n",
    "            \n",
    "def read_library(ppt, trial, w = 3.2, base_dsl_only=True, sort=False):\n",
    "    '''\n",
    "    Returns dsl learned by dreamcoder\n",
    "    '''\n",
    "    check_values(ppt, range(1, 50), 'ppt')\n",
    "    check_values(trial, range(1, 13), 'trial')\n",
    "    check_values(w, ws, 'w')\n",
    "    \n",
    "    lib = [parse(str(fragment), base_dsl_only=base_dsl_only) for fragment in dsls[ppt][trial][w_index[w]]]\n",
    "    \n",
    "    if sort:\n",
    "        lib = sorted(lib, key=lambda x: len(x.split()))\n",
    "    \n",
    "    return lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f66e47",
   "metadata": {},
   "source": [
    "Let's stick with the intermediate weight value for now (`w=3.3`).  \n",
    "\n",
    "We can use the `read_library()` function defined above to read in the library for participant 1 (`ppt = 1`) on the first trial (`trial=1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a dsl\n",
    "lib = read_library(ppt = 1, \n",
    "             trial = 1,\n",
    "             w = 3.3\n",
    "            )\n",
    "lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b050f",
   "metadata": {},
   "source": [
    "You should see an empty list. In trial 1, we've only seen one scene. Dreamcoder hasn't seen enough programs to identify any abstractions yet. But by their last trial (`trial=12`)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read another dsl\n",
    "lib = read_library(ppt = 1, \n",
    "             trial = 12,\n",
    "             w = 3.3\n",
    "            )\n",
    "lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4402d",
   "metadata": {},
   "source": [
    "Dreamcoder has learned 4 program fragments! We can also visualize these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [render_program(p) for p in lib]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99e5f1",
   "metadata": {},
   "source": [
    "This is what we hoped to see. Dreamcoder has learned program fragments for the towers that recurred across scenes. It also learned that smaller L shape. Can you work out why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069fa65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def rwt(p):\n",
    "# #     print(p.replace(' ','/'))\n",
    "# #     print(p)\n",
    "#     a = render_program(p)\n",
    "\n",
    "# _ = [rwt(b) for bs in [p for ps in libs for p in ps] for b in bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd64a3",
   "metadata": {},
   "source": [
    "## <span style=\"color: orange\"> Exercise: explore libraries across trials</span>\n",
    "\n",
    "We've just seen two endpoints for the library learning process for a single participant. We'd now like to get a more granular look at how libraries change over time. First try printing an entire sequence of libraries for one participant. Then, try to calculate some basic summary statistics across participants. Can you plot library size of trial? Over repetition? (the 12 trials were really split into 4 repetitions of each tower pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the series of libraries for a specific weight and participant\n",
    "\n",
    "[read_library(ppt = 1, \n",
    "             trial = trial,\n",
    "             w = 3.3) for trial in range(1,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in read_library(ppt = 15, \n",
    "#              trial = 12,\n",
    "#              w = 1.5,\n",
    "#              base_dsl_only=True):\n",
    "#     render_program(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the series of libraries for a specific weight and participant\n",
    "libs = [[read_library(ppt = ppt, \n",
    "             trial = trial,\n",
    "             w = 1.5,\n",
    "             base_dsl_only=False) for trial in range(1,13)] for ppt in range(1,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot library size\n",
    "lib_sizes = [[len(read_library(ppt = ppt, \n",
    "             trial = trial,\n",
    "             w = 3.3,\n",
    "             base_dsl_only=False)) for trial in range(1,13)] for ppt in range(1,50)]\n",
    "\n",
    "df = pd.DataFrame(lib_sizes)\n",
    "tidy_df = df.reset_index().melt(id_vars='index', var_name='trial', value_name='lib_size')\n",
    "tidy_df = tidy_df.rename(columns={'index': 'Participant'})\n",
    "sns.lineplot(data=tidy_df,\n",
    "             x=\"trial\",\n",
    "             y=\"lib_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e4d8c",
   "metadata": {},
   "source": [
    "### Exploring library contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314694f",
   "metadata": {},
   "source": [
    "We're currently displaying each fragment as a program in the base DSL, but the whole point of learning fragments is that you can express them as a single token.\n",
    "\n",
    "If we were fully automating this process, we might use an ID to label each learned fragment. In this example, only a handful of fragments are learned. This makes it possible to give each fragment a more helpful, human-readable name. If we change `base_dsl_only` to `False` then we can give these learned fragments helpful names (taken from a look-up table we've manually coded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_library(ppt = 1, \n",
    "             trial = 12,\n",
    "             w = 1.5,\n",
    "             base_dsl_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17524225",
   "metadata": {},
   "source": [
    "We refer to all these learned abstractions as \"chunks\". \n",
    "\n",
    "- `chunk_n`, where n is a number (and sometimes also a letter) are subtower expressions (capturing several block placements)\n",
    "- `chunk_Pi`, `chunk_C` and `chunk_L`, are distinct block towers. \n",
    "- `chunk_CPi` (and other chunks with two of C, L, and Pi) are single abstractions that capture the entire scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f78ddf",
   "metadata": {},
   "source": [
    "## <span style=\"color: orange\"> Exercise: compare libraries across different weights</span>\n",
    "\n",
    "First, see how library size and contents changes with different values of w. \n",
    "\n",
    "Next, see if you can get a sense of how the *content* of a library changes across weights. Is it the case that the same chunks are learned but at different rates, or are different kinds of chunks (at different abstraction levels) learned when the weighting parameter is diffferent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the series of libraries for a specific weight and participant\n",
    "libs = [[read_library(ppt = ppt, \n",
    "             trial = trial,\n",
    "             w = 1.5,\n",
    "             base_dsl_only=False) for trial in range(1,13)] for ppt in range(1,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deee70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([y for ys in [x for xs in libs for x in xs] for y in ys]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56431d4e",
   "metadata": {},
   "source": [
    "## Section 3: Representing scenes more concisely with refactored programs\n",
    "\n",
    "Now we have inferred the libraries of part concepts available to each participant in each trial. These part concepts, or program fragments, allow each scene program to be expressed more efficiently. In the next notebook, the model makes a decision about whether to use the most efficient program available to it, or play it safe and use lower-level language (about individual blocks). Before we get there, we first need to find these efficient programs.\n",
    "\n",
    "How to search for programs given a DSL is an interesting research question in itself. Here, however, our focus is on how people choose between more or less efficient ways of expressing a concept, given uncertainty about what their partner will understand. For our purposes, we just need to select the single most efficient program that represents each scene. Fortunately for us, Dreamcoder libraries uniquely determine this most efficient programs: we simply swap in the learned abstractions from our DSLs through string-matching. The `refactor_programs()` function does this refactoring for you.\n",
    "\n",
    "From here on out we're going to stick with `w=3.3`, as we found that this learning rate led to abstractions that most closely mirrored the expressions that people used across the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 3.3 # stick with 3.3 from here on\n",
    "w_position = w_index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refactorPrograms import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99764c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor the programs\n",
    "refactor_programs(dsls,\n",
    "                  trial_seqs,\n",
    "                  w_position = w_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect programs\n",
    "\n",
    "ppt = 1\n",
    "\n",
    "ppt_data = pd.read_json('../data/model/your_programs/programs_ppt_{}.json'.format(ppt))\n",
    "ppt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb505b0b",
   "metadata": {},
   "source": [
    "## <span style=\"color: orange\"> Exercise: Explore how programs change across trials</span>\n",
    "\n",
    "We now have, for each participant, the most concise way of expressing each scene program (min_program). Take a look at these programs and try to make sense of them. Do they get longer or shorter over trials?\n",
    "\n",
    "**Hint**: This exploration is analagous to the message length analysis in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2983747",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt = 1\n",
    "\n",
    "all_program_data = pd.DataFrame()\n",
    "\n",
    "for ppt in range(1,50):\n",
    "\n",
    "    with open(\"../data/model/your_programs/programs_ppt_\" + str(ppt) + \".json\", \"r\") as read_file:\n",
    "        trial_data = pd.read_json(read_file)\n",
    "        all_program_data = pd.concat([all_program_data, trial_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program_data['min_prog_length'] = all_program_data.min_program.apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "sns.lineplot(data = all_program_data,\n",
    "         x = 'trial_num',\n",
    "         y = 'min_prog_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program_data['rep'] = all_program_data.trial_num.apply(lambda n: np.ceil(n/3))\n",
    "\n",
    "\n",
    "sns.lineplot(data = all_program_data,\n",
    "         x = 'rep',\n",
    "         y = 'min_prog_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68d2b4",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We've seen how we can model the learning of part concepts as adding program fragments to a library. We've seen that doing so allows us to express a scene program more concisely. Those keen-eyed among you may have spotted that `refactor_programs()` also outputted another variable `programs_with_length`. This contains a small range of programs, derived from the shortest program, that use more or less of these fragments, and hence **express the same program with a range of levels of concision**. More concise programs use more abstractions. In the [final notebook](/notebooks/conventions.ipynb), we explore how people might weigh this concision against another factor-- informativity.\n",
    "\n",
    "Woohoo! Notebook 2 complete! Have a stretch, go for a walk, drink some water.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8abf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_env",
   "language": "python",
   "name": "ca_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
